# KompressAI

## Problem Statement

-Deploying large-scale AI models on low-power or embedded devices is a major challenge.

- These models require high computational power and large memory capacity.

- They often depend on continuous cloud access, limiting their use in remote or edge-based environments.

- This makes them unsuitable for cost-sensitive applications in industries such as agriculture, robotics, and healthcare.

- On-device intelligence is essential in these sectors for real-time decision-making and reliability.

- KompressAI addresses these challenges by enabling efficient and accessible AI model deployment.

- It directly supports Sustainable Development Goal (SDG) 9 by promoting innovation and fostering sustainable infrastructure.

## Objectives
- Enable efficient AI model deployment on resource-constrained hardware.
- Simplify compression through a no-code, user-friendly interface.
- Reduce power consumption and latency without sacrificing accuracy.
- Democratize AI deployment for innovators, educators, and small-scale developers.
## Solution
- We made a website where users are able to create projects and submit their AI models
- then they are able to tell us what hardware and how they want their AI models to compress. These are the following features we include
  - Hardware
    - GPU's
    - CPU's
    - MCU's
    - Edge Devices
  - Optimization Level
    - Lantency(to increase speed of processing)
    - Size(decrease size of models)
    - Accuracy (focus more on retaining accuracy)
- Bassed on these the usser chooses any commbination of the two that suits his needs best
- then we use our in house built AI model to predict the best way to achive the best compression for the give situation
- our AI  is able to understandwhat exactlly the model needs so if someone has access to good hardware and wants to increaase accuracy or latency our AI model understands it and can improve the performance of the model by increasing its size and reducing latency or increassing accuracy
- and when thier are thgiht constraints on hardware our model can reduce the size of the users model by 70% while retaaining  accruacy within a 0.001% margin and without increasing latency at all

## How It Works
   
    - 
